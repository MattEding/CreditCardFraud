{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / 'data'\n",
    "pkl_dir = data_dir / 'pkl'\n",
    "\n",
    "credit_test = pd.read_pickle(pkl_dir / 'credit_test.pkl')\n",
    "X_test, y_test = credit_test.drop(columns=['Class']), credit_test['Class']\n",
    "\n",
    "with open(pkl_dir / 'smote_best.pkl', 'rb') as fp:\n",
    "    smote_best = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "with open(pkl_dir / 'adasyn_best.pkl', 'rb') as fp:\n",
    "    adasyn_best = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "My metric of choice is recall, but I will also look into precision as the number of false-positives will affect business decisions should auditing too many non-fraud accounts be more costly than missing a few fraud accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE\n",
      "Recall: 94.06%\n",
      "[[55436  1425]\n",
      " [    6    95]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_smote = smote_best.predict(X_test)\n",
    "recall_smote = recall_score(y_test, y_pred_smote)\n",
    "conf_mtx_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "print('SMOTE')\n",
    "print(f'Recall: {recall_smote:.2%}')\n",
    "print(conf_mtx_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADASYN\n",
      "Recall: 97.03%\n",
      "[[51404  5457]\n",
      " [    3    98]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_adasyn = adasyn_best.predict(X_test)\n",
    "recall_adasyn = recall_score(y_test, y_pred_adasyn)\n",
    "conf_mtx_adasyn = confusion_matrix(y_test, y_pred_adasyn)\n",
    "print('ADASYN')\n",
    "print(f'Recall: {recall_adasyn:.2%}')\n",
    "print(conf_mtx_adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $F$ be the cost of leaving a fraudulent account being uncaught.\n",
    "\n",
    "Let $f$ be the number of uncaught fraudulent accounts (TN).\n",
    "\n",
    "Let $A$ be the cost of auditing an account to see if it is fraudulent.\n",
    "\n",
    "Let $a$ be the number of accounts flagged as fraudulent (TP + FP).\n",
    "\n",
    "\n",
    "$$\\text{SMOTE } = \\text{ADASYN}$$\n",
    "\n",
    "$$F \\cdot f_{\\tiny{SMOTE}} + A \\cdot a_{\\tiny{SMOTE}} = F \\cdot f_{\\tiny{ADASYN}} + A \\cdot a_{\\tiny{ADASYN}}$$\n",
    "\n",
    "$$6F + 1520A = 3F + 5555A$$\n",
    "\n",
    "$$3F = 4035A$$\n",
    "\n",
    "$$F = 1345A$$\n",
    "\n",
    "\n",
    "Thus if the cost of leaving a fraudulent account uncaught is ~1350 times the cost per audit, the ADASYN model should be the one used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_cost/A_cost = 100000/50 = 200,000.00%\n",
      "SMOTE cost: $676,000\n",
      "ADASYN cost: $577,750\n",
      "Use ADASYN: True\n"
     ]
    }
   ],
   "source": [
    "audit = 50\n",
    "fraud = audit * 2000\n",
    "\n",
    "print(f'F_cost/A_cost = {fraud}/{audit} = {fraud / audit:,.2%}')\n",
    "\n",
    "smote_cost = 6 * fraud + 1520 * audit\n",
    "print(f'SMOTE cost: ${smote_cost:,}')\n",
    "\n",
    "adasyn_cost = 3 * fraud + 5555 * audit\n",
    "print(f'ADASYN cost: ${adasyn_cost:,}')\n",
    "\n",
    "print(f'Use ADASYN: {adasyn_cost < smote_cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
